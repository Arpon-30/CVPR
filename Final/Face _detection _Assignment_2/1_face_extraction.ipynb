{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1cf06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0\n",
      "No face: IMG_20250605_151229.jpg\n",
      "No face: IMG_20250926_091238.jpg\n",
      "No face: IMG_20251116_150437.jpg\n",
      "No face: IMG_20251217_220756(1).jpg\n",
      "No face: IMG_20251217_220756.jpg\n",
      "No face: IMG_20251217_220917(1).jpg\n",
      "No face: IMG_20251217_220917.jpg\n",
      "No face: IMG_20251217_220921_1(1).jpg\n",
      "No face: IMG_20251217_220921_1.jpg\n",
      "No face: IMG_20251217_220958.jpg\n",
      "No face: IMG_20251217_221150.jpg\n",
      "No face: IMG_20251217_221205.jpg\n",
      "No face: IMG_20251217_221207.jpg\n",
      "No face: IMG_20251217_221220.jpg\n",
      "No face: IMG_20251217_221222.jpg\n",
      "No face: IMG_20251217_221227.jpg\n",
      "No face: IMG_20251217_221230.jpg\n",
      "No face: IMG_20251217_221240.jpg\n",
      "No face: IMG_20251217_221242.jpg\n",
      "No face: IMG_20251217_221343.jpg\n",
      "No face: IMG_20251217_221351.jpg\n",
      "No face: IMG_20251217_221400.jpg\n",
      "No face: IMG_20251217_222417.jpg\n",
      "Saved 164 faces\n",
      "\n",
      "Processing: 1\n",
      "No face: PXL_20241216_104617691.MP.jpg\n",
      "No face: PXL_20241216_104622268.MP.jpg\n",
      "No face: PXL_20241216_104626759.MP.jpg\n",
      "No face: PXL_20241216_104627533.jpg\n",
      "No face: PXL_20241216_104628960.MP.jpg\n",
      "No face: PXL_20241216_104632573.MP.jpg\n",
      "No face: PXL_20241216_140219211.jpg\n",
      "No face: PXL_20241216_140229871.jpg\n",
      "No face: PXL_20241216_140232800.jpg\n",
      "No face: PXL_20241216_140234963.jpg\n",
      "No face: PXL_20241216_140259479.jpg\n",
      "No face: PXL_20241216_140310189.MP.jpg\n",
      "No face: PXL_20241216_140314470.jpg\n",
      "No face: PXL_20241216_140316848.MP.jpg\n",
      "No face: PXL_20241216_140416816.MP.jpg\n",
      "No face: PXL_20241216_140418184.jpg\n",
      "No face: PXL_20241216_140833365.jpg\n",
      "No face: PXL_20241216_140834038.jpg\n",
      "No face: PXL_20241216_140834532.jpg\n",
      "No face: PXL_20241216_140834905.jpg\n",
      "No face: PXL_20241216_140837256.jpg\n",
      "No face: PXL_20241216_140838033.jpg\n",
      "No face: PXL_20241216_140845372.jpg\n",
      "No face: PXL_20241217_021511730.MP.jpg\n",
      "No face: PXL_20241217_021513178.MP.jpg\n",
      "No face: PXL_20241217_021540761.jpg\n",
      "No face: PXL_20241217_021548017.MP.jpg\n",
      "No face: PXL_20241217_131600773.NIGHT.jpg\n",
      "No face: PXL_20241217_131749948.NIGHT.jpg\n",
      "No face: PXL_20241217_131754717.NIGHT.jpg\n",
      "No face: PXL_20241217_131809225.NIGHT.jpg\n",
      "No face: PXL_20241217_131821616.NIGHT.jpg\n",
      "No face: PXL_20241217_131827617.NIGHT.jpg\n",
      "No face: PXL_20241217_131832289.NIGHT.jpg\n",
      "No face: PXL_20241218_034152948.MP.jpg\n",
      "No face: PXL_20241218_034156037.MP.jpg\n",
      "No face: PXL_20241218_034159231.MP.jpg\n",
      "No face: PXL_20241218_052012026.jpg\n",
      "No face: PXL_20241218_052037182.jpg\n",
      "No face: PXL_20241218_052044317.jpg\n",
      "No face: PXL_20250829_055612661.NIGHT.jpg\n",
      "Saved 100 faces\n",
      "\n",
      "Processing: 2\n",
      "No face: 20250704_115027.jpg\n",
      "No face: 20251220_165359.jpg\n",
      "No face: 20251220_165422.jpg\n",
      "No face: 20251220_165452.jpg\n",
      "No face: 20251220_165508.jpg\n",
      "No face: 20251220_165517.jpg\n",
      "No face: 20251220_165601.jpg\n",
      "No face: 20251220_165612.jpg\n",
      "No face: 20251220_165637.jpg\n",
      "No face: 20251220_165640.jpg\n",
      "No face: 20251220_165709.jpg\n",
      "No face: 20251220_165719.jpg\n",
      "No face: 20251220_165727.jpg\n",
      "No face: 20251220_165749.jpg\n",
      "No face: 20251220_165753.jpg\n",
      "No face: 20251220_165813.jpg\n",
      "No face: 20251220_165815.jpg\n",
      "No face: 20251220_165819.jpg\n",
      "No face: 20251221_010005.jpg\n",
      "No face: 20251221_010015.jpg\n",
      "No face: 20251221_010023.jpg\n",
      "No face: 20251221_010038.jpg\n",
      "No face: 20251221_010042.jpg\n",
      "No face: 20251221_010108.jpg\n",
      "No face: 20251221_010111.jpg\n",
      "No face: 20251221_010119.jpg\n",
      "No face: 20251221_010122.jpg\n",
      "No face: 20251221_010134.jpg\n",
      "No face: 20251221_010139.jpg\n",
      "No face: 20251221_010146.jpg\n",
      "No face: 20251221_010155.jpg\n",
      "No face: 20251221_010159.jpg\n",
      "No face: 20251221_010202.jpg\n",
      "Saved 68 faces\n",
      "\n",
      "All faces extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "INPUT_DIR = \"Dataset\"\n",
    "OUTPUT_DIR = \"Dataset_Faces\"\n",
    "IMG_SIZE = (224, 224)\n",
    "MIN_FACE_SIZE = (80, 80)\n",
    "\n",
    "# Load Haar Cascade\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "# Create output base directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through each person folder\n",
    "for person_name in os.listdir(INPUT_DIR):\n",
    "    person_path = os.path.join(INPUT_DIR, person_name)\n",
    "\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing: {person_name}\")\n",
    "\n",
    "    # Create output folder for this person\n",
    "    out_person_dir = os.path.join(OUTPUT_DIR, person_name)\n",
    "    os.makedirs(out_person_dir, exist_ok=True)\n",
    "\n",
    "    img_count = 0\n",
    "\n",
    "    # Loop through images\n",
    "    for img_name in os.listdir(person_path):\n",
    "        if not img_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=MIN_FACE_SIZE\n",
    "        )\n",
    "\n",
    "        # If no face found, skip\n",
    "        if len(faces) == 0:\n",
    "            print(f\"No face: {img_name}\")\n",
    "            continue\n",
    "\n",
    "        # Take the largest face \n",
    "        faces = sorted(faces, key=lambda x: x[2]*x[3], reverse=True)\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, IMG_SIZE)\n",
    "\n",
    "        save_name = f\"{person_name}_{img_count}.jpg\"\n",
    "        save_path = os.path.join(out_person_dir, save_name)\n",
    "        cv2.imwrite(save_path, face)\n",
    "\n",
    "        img_count += 1\n",
    "\n",
    "    print(f\"Saved {img_count} faces\\n\")\n",
    "\n",
    "print(\"All faces extracted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43bae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90:10 train-test split\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Dataset_Faces\\\\0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     42\u001b[0m     cls_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(INPUT_DIR, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m     images \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m               \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[0;32m     46\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(images)\n\u001b[0;32m     47\u001b[0m     split_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m*\u001b[39m TRAIN_RATIO)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Dataset_Faces\\\\0'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "INPUT_DIR = \"Dataset_Faces\"\n",
    "OUTPUT_DIR = \"Final_Dataset\"\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "TARGET_TRAIN_COUNT = 520  \n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def augment(img):\n",
    "    aug = []\n",
    "\n",
    "    # flip\n",
    "    aug.append(cv2.flip(img, 1))\n",
    "\n",
    "    # rotations\n",
    "    for angle in [-10, 10]:\n",
    "        M = cv2.getRotationMatrix2D((112,112), angle, 1.0)\n",
    "        aug.append(cv2.warpAffine(img, M, IMG_SIZE))\n",
    "\n",
    "    # brightness\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    hsv[:,:,2] = np.clip(hsv[:,:,2] * 1.2, 0, 255)\n",
    "    aug.append(cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR))\n",
    "\n",
    "    return aug\n",
    "\n",
    "# TRAIN - TEST SPLIT \n",
    "print(\"90:10 train-test split\")\n",
    "\n",
    "for cls in [\"0\", \"1\", \"2\"]:\n",
    "    cls_path = os.path.join(INPUT_DIR, cls)\n",
    "    images = [f for f in os.listdir(cls_path)\n",
    "              if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))]\n",
    "\n",
    "    random.shuffle(images)\n",
    "    split_idx = int(len(images) * TRAIN_RATIO)\n",
    "\n",
    "    train_imgs = images[:split_idx]\n",
    "    test_imgs  = images[split_idx:]\n",
    "\n",
    "    for split, img_list in zip([\"train_raw\", \"test\"], [train_imgs, test_imgs]):\n",
    "        out_dir = os.path.join(OUTPUT_DIR, split, cls)\n",
    "        ensure_dir(out_dir)\n",
    "\n",
    "        for img in img_list:\n",
    "            shutil.copy(\n",
    "                os.path.join(cls_path, img),\n",
    "                os.path.join(out_dir, img)\n",
    "            )\n",
    "\n",
    "print(\"Train-Test split done\\n\")\n",
    "\n",
    "# OFFLINE AUGMENT TRAIN \n",
    "print(\"Offline augmentation to 520 images/class\")\n",
    "\n",
    "RAW_TRAIN = os.path.join(OUTPUT_DIR, \"train_raw\")\n",
    "FINAL_TRAIN = os.path.join(OUTPUT_DIR, \"train\")\n",
    "\n",
    "for cls in [\"0\", \"1\", \"2\"]:\n",
    "    in_dir = os.path.join(RAW_TRAIN, cls)\n",
    "    out_dir = os.path.join(FINAL_TRAIN, cls)\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    imgs = [f for f in os.listdir(in_dir)\n",
    "            if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))]\n",
    "\n",
    "    saved = 0\n",
    "\n",
    "    # Save originals first\n",
    "    for img_name in imgs:\n",
    "        img = cv2.imread(os.path.join(in_dir, img_name))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        cv2.imwrite(os.path.join(out_dir, img_name), img)\n",
    "        saved += 1\n",
    "\n",
    "    idx = 0\n",
    "    while saved < TARGET_TRAIN_COUNT:\n",
    "        img_name = imgs[idx % len(imgs)]\n",
    "        img = cv2.imread(os.path.join(in_dir, img_name))\n",
    "        if img is None:\n",
    "            idx += 1\n",
    "            continue\n",
    "\n",
    "        for aug in augment(img):\n",
    "            if saved >= TARGET_TRAIN_COUNT:\n",
    "                break\n",
    "            save_name = f\"aug_{saved}.jpg\"\n",
    "            cv2.imwrite(os.path.join(out_dir, save_name), aug)\n",
    "            saved += 1\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    print(f\"Class {cls}: {saved} train images created\")\n",
    "\n",
    "print(\"\\nFINAL DATASET READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33641ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
